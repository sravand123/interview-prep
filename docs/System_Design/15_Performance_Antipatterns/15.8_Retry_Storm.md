---
topic: System Design
section: Performance Antipatterns
subtopic: Retry Storm
level: Advanced
---

## Retry Storm
### Core Concepts

*   **Retry Storm:** An uncontrolled cascade of retries originating from a single transient failure. When a service or client encounters a temporary error, it retries the operation. If the underlying issue persists or the retry mechanism is too aggressive, this can lead to a flood of requests overwhelming the system, exacerbating the original problem.

### Key Details & Nuances

*   **Root Cause:** Often triggered by transient network issues, temporary resource unavailability, or service overload.
*   **Amplification:** The problem isn't just the initial failure, but the subsequent *multiplication* of requests hitting the failing component.
*   **Impact:**
    *   **Increased Latency:** Surviving components become bogged down.
    *   **System Instability:** Can lead to cascading failures.
    *   **Resource Exhaustion:** CPU, memory, network bandwidth, connection pools.
*   **Client-Side vs. Server-Side:**
    *   **Client-Side:** A single client retrying excessively.
    *   **Server-Side (Less Common but Possible):** A server component retrying internal operations too aggressively.
*   **Backoff Strategies:** Crucial for mitigating retry storms.
    *   **Fixed Backoff:** Simple, but can still lead to synchronized retries.
    *   **Exponential Backoff:** Increases delay between retries (e.g., 1s, 2s, 4s, 8s). Significantly reduces the chance of a storm.
    *   **Jitter:** Adding a random delay to exponential backoff to desynchronize retries. **Essential** for production.

### Practical Examples

A simple client-side retry mechanism without proper backoff or jitter:

```typescript
async function fetchData(url: string): Promise<any> {
    try {
        const response = await fetch(url);
        if (!response.ok) {
            // Simulate transient error
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
    } catch (error) {
        console.error("Failed to fetch, retrying...");
        // NO BACKOFF OR JITTER - very likely to cause a retry storm
        return fetchData(url);
    }
}
```

**Improved version with exponential backoff and jitter:**

```typescript
async function fetchDataWithRetry(url: string, retries: number = 3, delay: number = 1000): Promise<any> {
    try {
        const response = await fetch(url);
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
    } catch (error) {
        if (retries <= 0) {
            console.error("Max retries reached. Giving up.");
            throw error;
        }
        console.warn(`Failed to fetch: ${error}. Retrying in ${delay}ms...`);

        // Add jitter
        const jitter = Math.random() * delay * 0.5; // e.g., up to 50% of delay
        const nextDelay = delay * 2 + jitter; // Exponential backoff + jitter

        await new Promise(resolve => setTimeout(resolve, delay + jitter));
        return fetchDataWithRetry(url, retries - 1, nextDelay);
    }
}
```

### Common Pitfalls & Trade-offs

*   **Blind Retries:** Retrying operations without understanding the root cause or implementing backoff.
*   **Aggressive Retry Limits:** Setting the number of retries too high can prolong the agony.
*   **Synchronized Retries:** Multiple clients retrying at the exact same interval after a failure. Jitter is the solution.
*   **Retry on Non-Transient Errors:** Retrying permanent errors (e.g., 404 Not Found, 401 Unauthorized) is futile and wastes resources.
*   **Trade-off: Retry Complexity vs. Robustness:** Implementing sophisticated retry mechanisms (exponential backoff, jitter, circuit breakers) adds code complexity but significantly improves resilience.
*   **Trade-off: Latency vs. Availability:** Aggressive retries can mask underlying issues, leading to longer overall latency for all requests, but might provide higher availability for some operations if the transient issue is short-lived.

### Interview Questions

1.  **Q: Describe a "Retry Storm" and how you would prevent it in a distributed system.**
    *   **A:** A retry storm is when a transient failure causes a flood of retries, overwhelming the system. Prevention involves robust retry strategies: **exponential backoff** to increase delay between attempts and **jitter** (adding a random delay) to desynchronize retries from different clients. Additionally, implementing **circuit breakers** can stop retries altogether when a service is known to be unhealthy, preventing further load. Limiting the maximum number of retries is also crucial.

2.  **Q: When should a client *not* retry an operation?**
    *   **A:** A client should not retry operations that result in **non-transient errors**. Examples include:
        *   `4xx` client errors like `400 Bad Request`, `401 Unauthorized`, `403 Forbidden`, `404 Not Found`. These indicate a client-side issue or a permanent resource problem that retrying won't fix.
        *   `500 Internal Server Error` *can* sometimes be transient, but if it's consistently returned, it might indicate a deeper, unrecoverable issue on the server side, and continued retries could be harmful. It depends on the specific error code semantics and system knowledge.

3.  **Q: What is the role of jitter in a retry mechanism?**
    *   **A:** Jitter is a random variation added to the delay period between retries. Its primary role is to **desynchronize retries** originating from multiple clients. Without jitter, if many clients experience a failure simultaneously, they might all retry at the same intervals, creating synchronized bursts of traffic that can re-trigger the failure or overload the system. Jitter spreads these retries out, creating a smoother load profile and increasing the chance of success for individual retries.

4.  **Q: How might a retry storm manifest in a microservices architecture?**
    *   **A:** In a microservices architecture, a retry storm can occur when one service depends on another. If the dependent service experiences a transient failure, clients of the *first* service might retry their requests to the first service. If the first service itself is retrying its calls to the *second* service, and its own clients are retrying their calls to it, the storm can amplify rapidly. This can quickly degrade performance and stability across multiple services. Implementing retry logic with backoff and jitter at each service-to-service communication boundary is essential. Circuit breakers are also highly effective here.