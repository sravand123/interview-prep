---
topic: System Design
section: Performance Antipatterns
subtopic: No Caching
level: Advanced
---

## No Caching
### Core Concepts

*   **No Caching:** A performance antipattern where frequently accessed data or computation results are *not* stored (cached) for faster retrieval upon subsequent requests.
*   **Impact:** Leads to repeated expensive operations (e.g., database queries, complex computations, network calls), increasing latency, resource utilization, and potential for system overload.
*   **Inversion:** The opposite of good caching practices; implies a disregard for optimizing read operations.

### Key Details & Nuances

*   **Repeated Work:** The core issue is performing the same work multiple times when it could be avoided.
*   **Resource Consumption:**
    *   **CPU:** Re-computing results.
    *   **I/O:** Repeatedly fetching data from disk or network.
    *   **Network Bandwidth:** Fetching the same data from remote services.
*   **Scalability Bottleneck:** Systems without caching often hit performance limits sooner as load increases, as each request incurs full processing cost.
*   **Impact on User Experience:** Higher latency directly translates to a poorer user experience.

### Practical Examples

*   **Scenario:** An e-commerce product page that repeatedly queries the database for product details, price, and inventory status on *every* page load, even for popular items.

    ```mermaid
    graph TD;
        A["User requests product page"] --> B["Server fetches product details"];
        B --> C["Database query for product"];
        C --> B;
        B --> D["Server fetches price"];
        D --> E["Database query for price"];
        E --> D;
        B --> F["Server fetches inventory"];
        F --> G["Database query for inventory"];
        G --> F;
        B --> H["Renders product page"];
        H --> A;
    ```
*   **Anti-pattern:** If a product is viewed hundreds of times, the database is hit hundreds of times for the same data.
*   **Contrast (with caching):** A cache (e.g., Redis, Memcached, in-memory) would store product details. The first request hits the database, subsequent requests hit the cache directly.

### Common Pitfalls & Trade-offs

*   **Under-caching:** The explicit antipattern.
*   **Over-caching/Stale Data:** While not "no caching," it's a related pitfall. Caching without proper invalidation or expiration leads to serving stale data, which can be as bad or worse than slow data. This is *not* "no caching" but a failure in cache *management*.
*   **Cache Invalidation Complexity:** The primary trade-off against caching is the complexity of managing cache validity. Deciding *when* and *how* to update cached data (e.g., Time-To-Live (TTL), write-through, write-behind, explicit invalidation) adds engineering effort.
*   **Cache Stampede:** If many requests for an uncached item arrive simultaneously, they all miss the cache and hit the origin. If caching were in place, a mechanism to prevent all requests from re-fetching the same data (e.g., locking, probabilistic early expiration) would be needed. "No caching" avoids this specific problem by doing the expensive work every time.

### Interview Questions

*   **Question:** "Describe a situation where you observed a system performance issue and how you diagnosed it. Did caching play a role?"
    *   **Answer:** "In a past project, a dashboard experienced slow load times. By monitoring network requests and backend logs, I identified that the API was performing expensive, repeated database aggregations on every request. There was no caching layer for these dashboard metrics. Implementing an in-memory cache with a short TTL (e.g., 5 minutes) significantly reduced database load and improved response times from several seconds to milliseconds."

*   **Question:** "What are the primary performance bottlenecks you'd expect in a web application that *doesn't* use any form of caching for frequently accessed data?"
    *   **Answer:** "The main bottlenecks would be: 1. **Database Load:** Excessive read operations on the database. 2. **CPU Utilization:** Repeatedly computing or processing data that could be pre-computed. 3. **Network Latency:** Fetching data from external services or remote databases unnecessarily. This all contributes to higher end-to-end latency for users and reduced system throughput."

*   **Question:** "If you were designing a new service that serves user profile data, why would you consider caching, and what are the immediate downsides you'd be concerned about?"
    *   **Answer:** "I'd consider caching because user profile data is often read-heavy and changes infrequently. Caching it (e.g., in Redis or a local memory cache) would dramatically reduce database load and improve response times for profile lookups. The immediate downside to consider is **cache invalidation**: ensuring that when a user updates their profile, the cache is updated or invalidated correctly to prevent serving stale data. This introduces complexity in the write path."