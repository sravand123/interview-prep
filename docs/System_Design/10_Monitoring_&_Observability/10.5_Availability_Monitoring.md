---
topic: System Design
section: Monitoring & Observability
subtopic: Availability Monitoring
level: Advanced
---

## Availability Monitoring
### Core Concepts

*   **Availability Monitoring:** Continuously checking if a system or service is accessible and functioning as expected from an end-user's perspective.
*   **Goal:** Detect and alert on downtime or degraded performance as quickly as possible to minimize user impact and facilitate rapid recovery.
*   **Key Metrics:** Uptime percentage (e.g., 99.9%), response time, error rate.

### Key Details & Nuances

*   **Synthetic Monitoring (Probes/Checks):**
    *   Simulates user interactions or API calls from various locations (internal/external).
    *   **Check Types:**
        *   **Ping/HTTP(S) Checks:** Basic connectivity and response status.
        *   **API Endpoint Checks:** Verifies specific API functionality (e.g., GET `/users/{id}`).
        *   **Transaction Monitoring:** Simulates multi-step user workflows (e.g., login -> add to cart -> checkout).
    *   **Frequency:** How often checks are performed (trade-off between detection speed and load).
    *   **Location:** Where checks originate from (critical for distributed systems and identifying regional issues).
*   **Real User Monitoring (RUM):**
    *   Collects performance data directly from end-users' browsers or devices.
    *   Provides insights into actual user experience across different geographies, browsers, and network conditions.
    *   **Data Collected:** Page load times, API call durations, JavaScript errors, user journey analysis.
*   **Heartbeats:**
    *   A mechanism where a service periodically sends a "heartbeat" signal to a monitoring system to indicate it's alive and healthy.
    *   **Active vs. Passive:** Active (monitoring system polls service), Passive (service reports to monitoring system).
*   **Health Check Endpoints:**
    *   Dedicated API endpoints (e.g., `/health`, `/status`) within a service that provide information about its operational state.
    *   **Levels of Health:**
        *   **Liveness:** Is the service process running?
        *   **Readiness:** Is the service ready to accept traffic (e.g., dependencies initialized, database connected)?
        *   **Deep Health:** Checks all critical dependencies and internal states.
*   **Alerting:**
    *   Triggering notifications (email, Slack, PagerDuty) when availability metrics cross predefined thresholds.
    *   **Thresholds:** Define acceptable limits for response time, error rates, etc.
    *   **Downtime vs. Degraded Performance:** Differentiate between complete outages and performance issues that still allow basic functionality.
    *   **Alert Fatigue:** Managing the number and relevance of alerts to avoid missed critical incidents.

### Practical Examples

*   **Simple HTTP Check using `curl`:**

    ```bash
    curl --silent --output /dev/null --write-out "%{http_code}" https://your-service.com/health
    ```

    *   **Interpretation:** A `200 OK` indicates the endpoint is responding. Non-`200` codes signify potential issues.

*   **Synthetic Transaction Monitoring Logic (Conceptual):**

    ```typescript
    // Simulate a login and data retrieval transaction
    async function monitorUserLoginAndData() {
        const startTime = Date.now();
        let success = false;
        let error: Error | null = null;

        try {
            // Step 1: Login
            const loginResponse = await fetch("https://api.example.com/login", {
                method: "POST",
                body: JSON.stringify({ username: "testuser", password: "password" }),
                headers: { "Content-Type": "application/json" }
            });
            if (!loginResponse.ok) throw new Error("Login failed");
            const loginData = await loginResponse.json();
            const authToken = loginData.token;

            // Step 2: Fetch User Data
            const userDataResponse = await fetch("https://api.example.com/users/me", {
                headers: { "Authorization": `Bearer ${authToken}` }
            });
            if (!userDataResponse.ok) throw new Error("Fetch user data failed");
            await userDataResponse.json(); // Process user data

            success = true;
        } catch (e) {
            error = e as Error;
        } finally {
            const duration = Date.now() - startTime;
            // Report success/failure and duration to monitoring system
            reportAvailability("UserLoginAndData", success, duration, error);
        }
    }
    ```

*   **Health Check Endpoint Response (JSON):**

    ```json
    {
      "status": "UP",
      "checks": [
        {
          "name": "DatabaseConnection",
          "status": "UP",
          "duration": "50ms"
        },
        {
          "name": "CacheService",
          "status": "UP",
          "duration": "10ms"
        }
      ]
    }
    ```

### Common Pitfalls & Trade-offs

*   **Monitoring from Internal Network Only:** Can miss issues experienced by external users. **Trade-off:** Simpler setup vs. incomplete user perspective.
*   **Ignoring Degraded Performance:** Only alerting on complete outages misses usability issues. **Trade-off:** Alert noise vs. capturing all user impact.
*   **Infrequent Checks:** Slows down detection time. **Trade-off:** Faster detection vs. increased load on the monitored system and monitoring infrastructure.
*   **Overly Sensitive Alerts:** Too many false positives lead to alert fatigue. **Trade-off:** High availability awareness vs. actionable alerts.
*   **Monitoring the Monitor:** If the monitoring system itself is down, you won't know about other system failures.

### Interview Questions

1.  **How would you design an availability monitoring system for a global e-commerce website?**
    *   **Answer:** Implement a multi-layered approach. Use synthetic checks (HTTP, API, transaction) from diverse geographical locations to simulate user behavior and detect failures quickly. Supplement with RUM to capture real user experience. Utilize active health check endpoints within each service for liveness and readiness. Configure tiered alerts with clear thresholds for response time and error rates. Employ heartbeats for critical background processes. Ensure the monitoring infrastructure is highly available and scalable. Consider synthetic tests for key user journeys like search, add-to-cart, and checkout.

2.  **What's the difference between Liveness and Readiness probes, and why are they important for availability?**
    *   **Answer:** **Liveness probes** determine if a service instance is running and should be kept alive. If it fails, the orchestrator (e.g., Kubernetes) might restart the instance. **Readiness probes** determine if a service instance is ready to accept traffic. If it fails, the orchestrator will temporarily stop sending traffic to it until it becomes ready again. They are crucial for availability because they prevent traffic from being sent to unhealthy instances (readiness) and allow for automatic recovery of crashed instances (liveness), minimizing downtime.

3.  **How do you handle alert fatigue in a large, distributed system?**
    *   **Answer:** Focus on actionable alerts tied to real user impact. Implement tiered alerting: critical alerts for immediate action (e.g., site down), warning alerts for degraded performance that might lead to issues. Use smart aggregation and deduplication to reduce noise. Define clear Service Level Objectives (SLOs) and alert when SLOs are breached, not just when a raw metric crosses a threshold. Conduct regular post-mortems to identify and tune noisy alerts. Leverage anomaly detection to spot unusual patterns that might indicate a problem not covered by static thresholds.

4.  **When would you choose synthetic monitoring over Real User Monitoring (RUM), and vice-versa?**
    *   **Answer:** **Synthetic monitoring** is best for proactively checking uptime, basic functionality, and specific transaction success from controlled environments and locations, often before users notice. It's ideal for detecting outages. **RUM** is best for understanding the actual end-user experience, identifying performance bottlenecks across diverse user segments (geography, device, browser), and capturing errors that might only occur under specific real-world conditions. They are complementary; synthetic tells you *if* it's down, RUM tells you *how* it's performing for actual users.